{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# load the modules and functions\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../functions/')\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "from adjust_lon_xr_dataset import adjust_lon_xr_dataset\n",
    "from storm_interstorm_periods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove known warnings \n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate storm and interstorm statistics:\n",
    "\n",
    "- mean heat fluxes\n",
    "- mean air-sea temperature gradient\n",
    "- mean relative humidity\n",
    "- mean wind speed\n",
    "- storm and interstorm periods\n",
    "- number of interstorm periods in a season that were consecutively longer than 4 days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb02c556dd043239495d8cf8870d971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run the code that calculates the storm and interstorm days in the season\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "data_directory_lhf = glob('/Volumes/LaCie/Work/data/era5/DJF_1981_2023_daily_means/latent_heat_flux*.nc')\n",
    "data_directory_shf = glob('/Volumes/LaCie/Work/data/era5/DJF_1981_2023_daily_means/sensible_heat_flux*.nc')\n",
    "data_directory_ssr = glob('/Volumes/LaCie/Work/data/era5/DJF_1981_2023_daily_means/net_solar_radiation*.nc')\n",
    "data_directory_str = glob('/Volumes/LaCie/Work/data/era5/DJF_1981_2023_daily_means/net_thermal_radiation*.nc')\n",
    "data_directory_ws  = glob('/Volumes/LaCie/Work/data/era5/DJF_1981_2023_daily_means/winds*.nc')\n",
    "data_directory_sic = glob('/Volumes/LaCie/Work/data/era5/DJF_1981_2023_daily_means/sea_ice_cover*.nc')\n",
    "\n",
    "data_directory = sorted(data_directory_lhf + data_directory_shf + data_directory_ssr + data_directory_str + data_directory_ws + data_directory_sic)\n",
    "\n",
    "years = np.arange(1981, 2023, 1)\n",
    "\n",
    "storm_period = np.ndarray([years.size, 201, 1440]) * np.NaN\n",
    "interstorm_period = np.ndarray([years.size, 201, 1440]) * np.NaN\n",
    "\n",
    "storm_ws = np.ndarray([years.size, 201, 1440]) * np.NaN   \n",
    "interstorm_ws = np.ndarray([years.size, 201, 1440]) * np.NaN   \n",
    "\n",
    "storm_shf = np.ndarray([years.size, 201, 1440]) * np.NaN   \n",
    "interstorm_shf = np.ndarray([years.size, 201, 1440]) * np.NaN   \n",
    "\n",
    "storm_lhf = np.ndarray([years.size, 201, 1440]) * np.NaN   \n",
    "interstorm_lhf = np.ndarray([years.size, 201, 1440]) * np.NaN\n",
    "\n",
    "storm_str = np.ndarray([years.size, 201, 1440]) * np.NaN   \n",
    "interstorm_str = np.ndarray([years.size, 201, 1440]) * np.NaN\n",
    "\n",
    "storm_ssr = np.ndarray([years.size, 201, 1440]) * np.NaN   \n",
    "interstorm_ssr = np.ndarray([years.size, 201, 1440]) * np.NaN\n",
    "\n",
    "interstorm_period_total_days = np.ndarray([years.size, 201, 1440]) * np.NaN\n",
    "storm_period_total_days = np.ndarray([years.size, 201, 1440]) * np.NaN   \n",
    "\n",
    "for y in tqdm(range(years.size)):\n",
    "\n",
    "    # decide on the year to analyse\n",
    "    year = years[y]\n",
    "\n",
    "    # list the files for that given year\n",
    "    flist = [file for file in data_directory if file.endswith(str(year) + '_DJF.nc')]\n",
    "    \n",
    "    # read the files in with xarray\n",
    "    ds = xr.open_mfdataset(flist)\n",
    "\n",
    "    # load them into memory\n",
    "    ds = ds.load()\n",
    "\n",
    "    # remove the data points where there is sea ice\n",
    "\n",
    "    # Create a mask for sea ice concentration values less than 0.15\n",
    "    date_mask = ds.siconc < 0.15\n",
    "\n",
    "    # Create a replacement array with NaN values for the same shape as ds\n",
    "    replacement_array = xr.full_like(ds.siconc, fill_value=np.nan, dtype=float)\n",
    "\n",
    "    # Combine the original data and the replacement array\n",
    "    ds = replacement_array.combine_first(ds.where(date_mask))\n",
    "\n",
    "    # Calculate relative humidity (RH) using the formula and add it to the input dataset\n",
    "    \n",
    "    # Dp = ds.d2m.data - 273.15 # dew point temperature\n",
    "    # T  = ds.t2m.data - 273.15 # temperature in kelvin \n",
    "    \n",
    "    # ds['rh'] = (('time', 'latitude', 'longitude'), 100 * (np.exp((17.625 * Dp) / (243.04 + Dp)) / np.exp((17.625 * T) / (243.04 + T))))  \n",
    "    # ds['sh'] = (('time', 'latitude', 'longitude'), calculate_specific_humidity(T, ds['rh'].data))\n",
    "\n",
    "    # determine the storm and interstorm periods\n",
    "    \n",
    "    for i, ln in enumerate(ds.longitude.data):\n",
    "    \n",
    "        for ii, lt in enumerate(ds.latitude.data):\n",
    "\n",
    "            if (np.isnan(ds.siconc.sel(longitude=ln, latitude=lt).data)==False).sum() == ds.siconc.sel(longitude=ln, latitude=lt).data.size: # if 80% of the data is ice free, we analyse the whole dataset\n",
    "    \n",
    "                ds_gridcell = ds.sel(longitude=ln, latitude=lt)\n",
    "                \n",
    "                storm_indices, interstorm_indices = storm_interstorm_id(ds_gridcell['ws'].data, threshold=10)\n",
    "        \n",
    "                storm_period[y, ii, i] = np.median([len(s) for s in storm_indices])\n",
    "                interstorm_period[y, ii, i] = np.median([len(s) for s in interstorm_indices])\n",
    "\n",
    "                list_interstorm = np.array([len(s) for s in interstorm_indices]) \n",
    "                interstorm_period_total_days[y, ii, i] = list_interstorm.sum()\n",
    "\n",
    "                list_storm = np.array([len(s) for s in storm_indices]) \n",
    "                storm_period_total_days[y, ii, i] = list_storm.sum()\n",
    "\n",
    "                # define the storm and interstorm datasets\n",
    "\n",
    "                storm_list = [item for sublist in storm_indices for item in sublist]\n",
    "                interstorm_list = [item for sublist in interstorm_indices for item in sublist]\n",
    "\n",
    "                ds_storm = ds_gridcell.isel(time=storm_list)\n",
    "                ds_interstorm = ds_gridcell.isel(time=interstorm_list)\n",
    "\n",
    "                storm_ws[y, ii, i] = ds_storm.ws.mean(dim='time')\n",
    "                interstorm_ws[y, ii, i] = ds_interstorm.ws.mean(dim='time')\n",
    "\n",
    "                storm_shf[y, ii, i] = ds_storm.sshf.mean(dim='time')\n",
    "                interstorm_shf[y, ii, i] = ds_interstorm.sshf.mean(dim='time')\n",
    "\n",
    "                storm_lhf[y, ii, i] = ds_storm.slhf.mean(dim='time')\n",
    "                interstorm_lhf[y, ii, i] = ds_interstorm.slhf.mean(dim='time')\n",
    "\n",
    "                storm_str[y, ii, i] = ds_storm.str.mean(dim='time')\n",
    "                interstorm_str[y, ii, i] = ds_interstorm.str.mean(dim='time')\n",
    "\n",
    "                storm_ssr[y, ii, i] = ds_storm.ssr.mean(dim='time')\n",
    "                interstorm_ssr[y, ii, i] = ds_interstorm.ssr.mean(dim='time')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create xarray dataset\n",
    "ds_storms = xr.Dataset(\n",
    "    {\n",
    "        \"storm_period\": ([\"time\", \"latitude\", \"longitude\"], storm_period),\n",
    "        \"interstorm_period\": ([\"time\", \"latitude\", \"longitude\"], interstorm_period),\n",
    "        \n",
    "        \"interstorm_period_total_days\": ([\"time\", \"latitude\", \"longitude\"], interstorm_period_total_days),     \n",
    "        \"storm_period_total_days\": ([\"time\", \"latitude\", \"longitude\"], storm_period_total_days),   \n",
    "        \n",
    "        \"storm_ws\": ([\"time\", \"latitude\", \"longitude\"], storm_ws),             \n",
    "        \"interstorm_ws\": ([\"time\", \"latitude\", \"longitude\"], interstorm_ws),\n",
    "        \n",
    "        \"storm_shf\": ([\"time\", \"latitude\", \"longitude\"], storm_shf),             \n",
    "        \"interstorm_shf\": ([\"time\", \"latitude\", \"longitude\"], interstorm_shf),        \n",
    "\n",
    "        \"storm_lhf\": ([\"time\", \"latitude\", \"longitude\"], storm_lhf),             \n",
    "        \"interstorm_lhf\": ([\"time\", \"latitude\", \"longitude\"], interstorm_lhf),                \n",
    "\n",
    "        \"storm_ssr\": ([\"time\", \"latitude\", \"longitude\"], storm_ssr),             \n",
    "        \"interstorm_ssr\": ([\"time\", \"latitude\", \"longitude\"], interstorm_ssr),                \n",
    "\n",
    "        \"storm_str\": ([\"time\", \"latitude\", \"longitude\"], storm_str),             \n",
    "        \"interstorm_str\": ([\"time\", \"latitude\", \"longitude\"], interstorm_str),                \n",
    "          \n",
    "    },\n",
    "    coords={\n",
    "        \"time\": years,\n",
    "        \"latitude\": ds['latitude'],\n",
    "        \"longitude\": ds['longitude'],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the difference between storms and interstorms period\n",
    "\n",
    "var = ['shf', 'lhf', 'ssr', 'str']\n",
    "\n",
    "for v in var:\n",
    "\n",
    "    ds_storms['diff_' + v] = (('time', 'latitude', 'longitude'), ds_storms['storm_' + v].data - ds_storms['interstorm_' + v].data)\n",
    "\n",
    "ds_storms['diff_qnet'] = (('time', 'latitude', 'longitude'), ds_storms['diff_shf'].data + ds_storms['diff_lhf'].data + ds_storms['diff_ssr'].data + ds_storms['diff_str'].data)\n",
    "ds_storms['storm_qnet'] = (('time', 'latitude', 'longitude'), ds_storms['storm_shf'].data + ds_storms['storm_lhf'].data + ds_storms['storm_ssr'].data + ds_storms['storm_str'].data)\n",
    "ds_storms['interstorm_qnet'] = (('time', 'latitude', 'longitude'), ds_storms['interstorm_shf'].data + ds_storms['interstorm_lhf'].data + ds_storms['interstorm_ssr'].data + ds_storms['interstorm_str'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_storms.to_netcdf('../../data/era5/era5_storm_interstorm_periods_1981_2023_DJF.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storms_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
